{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid: # Environment\n",
    "  def __init__(self, width, height, start):\n",
    "    # i is vertical axis, j is horizontal\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0]\n",
    "    self.j = start[1]\n",
    "    self.start=start\n",
    "\n",
    "  def set(self, rewards, actions, obey_prob):\n",
    "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "    self.obey_prob = obey_prob\n",
    "\n",
    "  def non_terminal_states(self):\n",
    "    return self.actions.keys()\n",
    "\n",
    "  def terminal_states(self):\n",
    "    return [x for x in self.rewards.keys() if x not in self.actions.keys()]\n",
    "\n",
    "  def set_state(self, s):\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "\n",
    "  def current_state(self):\n",
    "    return (self.i, self.j)\n",
    "\n",
    "  def is_terminal(self, s):\n",
    "    return s not in self.actions\n",
    "\n",
    "  def stochastic_move(self, action):\n",
    "    p = np.random.random()\n",
    "    if p <= self.obey_prob:\n",
    "      return action\n",
    "    if action == 'U' or action == 'D':\n",
    "      return np.random.choice(['L', 'R'])\n",
    "    elif action == 'L' or action == 'R':\n",
    "      return np.random.choice(['U', 'D'])\n",
    "\n",
    "  def move(self, action):\n",
    "    actual_action = self.stochastic_move(action)\n",
    "    if actual_action in self.actions[(self.i, self.j)]:\n",
    "      if actual_action == 'U':\n",
    "        self.i -= 1\n",
    "      elif actual_action == 'D':\n",
    "        self.i += 1\n",
    "      elif actual_action == 'R':\n",
    "        self.j += 1\n",
    "      elif actual_action == 'L':\n",
    "        self.j -= 1\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "  def step(self,action):\n",
    "    actual_action=self.stochastic_move(action)\n",
    "    new_state=[self.i,self.j]\n",
    "    if actual_action in self.actions[(self.i, self.j)]:\n",
    "      if actual_action == 'U':\n",
    "        new_state[0] -= 1\n",
    "      elif actual_action == 'D':\n",
    "        new_state[0] += 1\n",
    "      elif actual_action == 'R':\n",
    "        new_state[1] += 1\n",
    "      elif actual_action == 'L':\n",
    "        new_state[1] -= 1\n",
    "    new_state=tuple(new_state)\n",
    "    reward=self.rewards[new_state]\n",
    "    done=self.is_terminal(new_state)\n",
    "    return new_state,reward,done\n",
    "\n",
    "  def check_move(self, action):\n",
    "    i = self.i\n",
    "    j = self.j\n",
    "    # check if legal move first\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        i -= 1\n",
    "      elif action == 'D':\n",
    "        i += 1\n",
    "      elif action == 'R':\n",
    "        j += 1\n",
    "      elif action == 'L':\n",
    "        j -= 1\n",
    "    return (i,j)\n",
    "    \n",
    "\n",
    "  def get_transition_probs(self, action):\n",
    "    # returns a list of (probability, s') transition tuples\n",
    "    probs = []\n",
    "    state = self.check_move(action)\n",
    "    probs.append((self.obey_prob, state))\n",
    "    disobey_prob = 1 - self.obey_prob\n",
    "    if not (disobey_prob > 0.0):\n",
    "      return probs\n",
    "    if action == 'U' or action == 'D':\n",
    "      state = self.check_move('L')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "      state = self.check_move('R')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "    elif action == 'L' or action == 'R':\n",
    "      state = self.check_move('U')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "      state = self.check_move('D')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "    return probs\n",
    "\n",
    "  def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if we are in a state where no actions are possible\n",
    "    return (self.i, self.j) not in self.actions\n",
    "\n",
    "  def all_states(self):\n",
    "    # possibly buggy but simple way to get all states\n",
    "    # either a position that has possible next actions\n",
    "    # or a position that yields a reward\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid(obey_prob=1.0, step_cost=None):\n",
    "  # define a grid that describes the reward for arriving at each state\n",
    "  # and possible actions at each state\n",
    "  # the grid looks like this\n",
    "  # x means you can't go there\n",
    "  # s means start position\n",
    "  # number means reward at that state\n",
    "  # .  .  .  1\n",
    "  # .  x  . -1\n",
    "  # s  .  .  .\n",
    "  # obey_brob (float): the probability of obeying the command\n",
    "  # step_cost (float): a penalty applied each step to minimize the number of moves (-0.1)\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions, obey_prob)\n",
    "  if step_cost is not None:\n",
    "    g.rewards.update({\n",
    "      (0, 0): step_cost,\n",
    "      (0, 1): step_cost,\n",
    "      (0, 2): step_cost,\n",
    "      (1, 0): step_cost,\n",
    "      (1, 2): step_cost,\n",
    "      (2, 0): step_cost,\n",
    "      (2, 1): step_cost,\n",
    "      (2, 2): step_cost,\n",
    "      (2, 3): step_cost,\n",
    "    })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_grid(obey_prob=1.0, step_cost=None):\n",
    "\n",
    "  g = Grid(8, 6, (7, 0))\n",
    "  rewards = {(1, 5): 1, (2, 5): -1,(2,3):0.5, (3,5):0.5}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'D','R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (0, 3): ('L', 'D', 'R'),\n",
    "    (0, 4): ('L', 'D', 'R'),\n",
    "    (0, 5): ('L', 'D'),\n",
    "    (1, 0): ('U', 'D','R'),\n",
    "    (1, 1): ('U', 'D', 'R','L'),\n",
    "    (1, 2): ('U', 'D', 'R','L'),\n",
    "    (1, 3): ('U', 'D', 'R','L'),\n",
    "    (1, 4): ('U', 'D', 'R','L'),       \n",
    "    (2, 0): ('U', 'D','R'),\n",
    "    (2, 1): ('U','L', 'R'),\n",
    "    (2, 2): ('U', 'D', 'R','L'),\n",
    "    (2, 3): ('U', 'D', 'R','L'),\n",
    "    (2, 4): ('U', 'D', 'R','L'),\n",
    "    (3, 0): ('U','D'),\n",
    "    (3, 2): ('U', 'D', 'R'),\n",
    "    (3, 3): ('U', 'D', 'R','L'),  \n",
    "    (3, 4): ('U', 'D', 'R','L'),    \n",
    "    (3, 5): ('U', 'D', 'L') , \n",
    "    (4, 0): ('U', 'D','R'),\n",
    "    (4, 1): ('D', 'R','L'),\n",
    "    (4, 2): ('U', 'D', 'R','L'),\n",
    "    (4, 3): ('U', 'D', 'R','L'),\n",
    "    (4, 4): ('U', 'D', 'R','L'),   \n",
    "    (4, 5): ('U', 'D', 'L'),   \n",
    "    (5, 0): ('U', 'D','R'),\n",
    "    (5, 1): ('U','D', 'R','L'),\n",
    "    (5, 2): ('U', 'D', 'R','L'),\n",
    "    (5, 3): ('U', 'D', 'R','L'),\n",
    "    (5, 4): ('U', 'D', 'R','L'),   \n",
    "    (5, 5): ('U', 'D', 'L'),  \n",
    "    (6, 0): ('U', 'D','R'),\n",
    "    (6, 1): ('U','D', 'R','L'),\n",
    "    (6, 2): ('U', 'D', 'R','L'),\n",
    "    (6, 3): ('U', 'D', 'R','L'),\n",
    "    (6, 4): ('U', 'D', 'R','L'),   \n",
    "    (6, 5): ('U', 'D', 'L'),       \n",
    "    (7, 0): ('U', 'R'),\n",
    "    (7, 1): ('U', 'R','L'),\n",
    "    (7, 2): ('U', 'R','L'),\n",
    "    (7, 3): ('U', 'R','L'),\n",
    "    (7, 4): ('U', 'R','L'),   \n",
    "    (7, 5): ('U', 'L')        \n",
    "  }\n",
    "\n",
    "  g.set(rewards, actions, obey_prob)\n",
    "  if step_cost is not None:\n",
    "    g.rewards.update({\n",
    "      (0, 0): step_cost,\n",
    "      (0, 1): step_cost,\n",
    "      (0, 2): step_cost,\n",
    "      (0, 3): step_cost,\n",
    "      (0, 4): step_cost,\n",
    "      (0, 5): step_cost,        \n",
    "      (1, 0): step_cost,\n",
    "      (1, 1): step_cost,\n",
    "      (1, 2): step_cost,\n",
    "      (1, 3): step_cost,\n",
    "      (1, 4): step_cost,  \n",
    "      (2, 0): step_cost,\n",
    "      (2, 1): step_cost,  \n",
    "      (2, 2): step_cost,\n",
    "      (2, 4): step_cost,\n",
    "      (3, 0): step_cost,\n",
    "      (3, 2): step_cost,\n",
    "      (3, 3): step_cost,\n",
    "      (3, 4): step_cost,  \n",
    "      (4, 0): step_cost,\n",
    "      (4, 1): step_cost,  \n",
    "      (4, 2): step_cost,\n",
    "      (4, 3): step_cost,\n",
    "      (4, 4): step_cost,\n",
    "      (4, 5): step_cost,  \n",
    "      (5, 0): step_cost,\n",
    "      (5, 1): step_cost,  \n",
    "      (5, 2): step_cost,\n",
    "      (5, 3): step_cost,\n",
    "      (5, 4): step_cost,\n",
    "      (5, 5): step_cost,  \n",
    "      (6, 0): step_cost,\n",
    "      (6, 1): step_cost,  \n",
    "      (6, 2): step_cost,\n",
    "      (6, 3): step_cost,\n",
    "      (6, 4): step_cost,\n",
    "      (6, 5): step_cost,\n",
    "      (7, 0): step_cost,\n",
    "      (7, 1): step_cost,  \n",
    "      (7, 2): step_cost,\n",
    "      (7, 3): step_cost,\n",
    "      (7, 4): step_cost,\n",
    "      (7, 5): step_cost,         \n",
    "\n",
    "    })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print('------------------------------------------------------------')\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print('')\n",
    "\n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print('------------------------------------------------------------')\n",
    "        for j in range(g.height):\n",
    "            a=P.get((i,j),' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print('')\n",
    "        \n",
    "def max_dict(d):\n",
    "    max_key=None\n",
    "    max_val=float('-inf')\n",
    "    for k,v in d.itmes():\n",
    "        if v>max_val:\n",
    "            max_val=v\n",
    "            max_key=k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH=1e-3\n",
    "GAMMA=0.9\n",
    "ALL_POSSIBLE_ACTIONS=('U', 'D', 'L', 'R')\n",
    "\n",
    "def policy_value(grid,V,s,P):\n",
    "    # finds the highest value action (max_a) from state s, returns the action and value\n",
    "    grid.set_state(s)\n",
    "    transititions=grid.get_transition_probs(P[s])\n",
    "    expected_v=0\n",
    "    for (prob,state_prime) in transititions:\n",
    "        expected_v +=prob*V[state_prime]\n",
    "    v=grid.rewards[s]+GAMMA*expected_v\n",
    "    return v\n",
    "\n",
    "def policy_iteration(grid):\n",
    "    policy=initialize_random_policy()\n",
    "    V={}\n",
    "    states=grid.all_states()\n",
    "    iterations=0\n",
    "    for s in states:\n",
    "        V[s]=grid.rewards[s]\n",
    "    while True:\n",
    "        iterations+=1\n",
    "        biggest_change=0\n",
    "        for s in grid.non_terminal_states():\n",
    "            old_v=V[s]\n",
    "            new_v=policy_value(grid,V,s,policy)\n",
    "            V[s]=new_v\n",
    "            biggest_change=max(biggest_change,np.abs(old_v-new_v))\n",
    "        if biggest_change<SMALL_ENOUGH:\n",
    "            break    \n",
    "        for s in grid.non_terminal_states():\n",
    "            grid.set_state(s)\n",
    "            max_value=float('-inf')\n",
    "            for m in grid.actions[s]:\n",
    "                v=0\n",
    "                T=grid.get_transition_probs(m)\n",
    "                for (prob,state_prime) in T:\n",
    "                    v +=prob*V[state_prime]\n",
    "                if v>max_value:\n",
    "                    max_value=v\n",
    "                    policy[s]=m\n",
    "    return V,iterations,policy\n",
    "\n",
    "def initialize_random_policy():\n",
    "    # policy is a lookup table for state -> action\n",
    "    # we'll randomly choose an action and update as we learn\n",
    "    policy={}\n",
    "    for s in grid.non_terminal_states():\n",
    "        policy[s]=np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "    return policy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "------------------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "------------------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "------------------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "values:\n",
      "------------------------------------\n",
      " 0.31| 0.51| 0.72| 1.00|\n",
      "------------------------------------\n",
      " 0.15| 0.00| 0.36|-1.00|\n",
      "------------------------------------\n",
      " 0.01| 0.01| 0.15|-0.09|\n",
      "policy:\n",
      "------------------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "------------------------------------\n",
      "  U  |     |  U  |     |\n",
      "------------------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "Iterations:  11\n"
     ]
    }
   ],
   "source": [
    "    grid=standard_grid(obey_prob=0.8,step_cost=-0.1)\n",
    "    \n",
    "    print('rewards:')\n",
    "    print_values(grid.rewards,grid)\n",
    "    \n",
    "    V,iteration,policy=policy_iteration(grid)\n",
    "        \n",
    "    print('values:')\n",
    "    print_values(V,grid)\n",
    "    print('policy:')\n",
    "    print_policy(policy,grid)\n",
    "    print('Iterations: ',iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03| 1.00|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03| 0.50|-0.03|-1.00|\n",
      "------------------------------------\n",
      "-0.03| 0.00|-0.03|-0.03|-0.03| 0.50|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "values:\n",
      "------------------------------------\n",
      " 0.97| 1.13| 1.30| 1.48| 1.30| 1.10|\n",
      "------------------------------------\n",
      " 1.11| 1.31| 1.53| 1.78| 1.53| 1.00|\n",
      "------------------------------------\n",
      " 1.25| 1.50| 1.78| 2.13| 1.81|-1.00|\n",
      "------------------------------------\n",
      " 1.06| 0.00| 1.55| 1.81| 1.90| 2.24|\n",
      "------------------------------------\n",
      " 0.94| 1.11| 1.33| 1.54| 1.65| 1.91|\n",
      "------------------------------------\n",
      " 0.80| 0.96| 1.13| 1.31| 1.42| 1.61|\n",
      "------------------------------------\n",
      " 0.68| 0.80| 0.96| 1.11| 1.21| 1.36|\n",
      "------------------------------------\n",
      " 0.57| 0.68| 0.80| 0.93| 1.03| 1.15|\n",
      "policy:\n",
      "------------------------------------\n",
      "  R  |  R  |  D  |  D  |  D  |  L  |\n",
      "------------------------------------\n",
      "  R  |  R  |  D  |  D  |  L  |     |\n",
      "------------------------------------\n",
      "  R  |  R  |  R  |  D  |  L  |     |\n",
      "------------------------------------\n",
      "  U  |     |  R  |  U  |  R  |  D  |\n",
      "------------------------------------\n",
      "  R  |  R  |  U  |  U  |  U  |  U  |\n",
      "------------------------------------\n",
      "  R  |  R  |  U  |  U  |  U  |  U  |\n",
      "------------------------------------\n",
      "  U  |  R  |  U  |  U  |  U  |  U  |\n",
      "------------------------------------\n",
      "  U  |  R  |  U  |  U  |  U  |  U  |\n",
      "Iterations:  35\n"
     ]
    }
   ],
   "source": [
    "    grid=big_grid(obey_prob=0.8,step_cost=-0.03)\n",
    "    \n",
    "    print('rewards:')\n",
    "    print_values(grid.rewards,grid)\n",
    "    \n",
    "    V,iteration,policy=policy_iteration(grid)\n",
    "    \n",
    "    print('values:')\n",
    "    print_values(V,grid)\n",
    "    print('policy:')\n",
    "    print_policy(policy,grid)\n",
    "    print('Iterations: ',iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huge_grid(obey_prob=1.0, step_cost=None):\n",
    "\n",
    "  g = Grid(10, 10, (9, 0))\n",
    "  rewards = {(1, 9): 1, (2, 9): -1,(4,2):-0.2, (5,5):0.2}\n",
    "  actions = {\n",
    "      (0,0): ('R','D'),\n",
    "      (0,9): ('L','D'),\n",
    "      (9,0): ('R','U'),\n",
    "      (9,9): ('L','U'),\n",
    "  }\n",
    "  for j in range(1,9):\n",
    "      actions[(0,j)]=('L','R','D')\n",
    "      actions[(9,j)]=('L','R','U')\n",
    "  for i in range(1,9):\n",
    "      actions[(i,0)]=('R','U','D')\n",
    "  for i in range(3,9):\n",
    "      actions[(i,9)]=('L','U','D')\n",
    "  for i in range(1,9):\n",
    "    for j in range(1,9):\n",
    "        actions[(i,j)]=('L','R','U','D')\n",
    "  actions.update({\n",
    "      (6,4): ('L','U','D'),\n",
    "      (5,5): ('L','R','U'),\n",
    "      (6,6): ('R','U','D'),\n",
    "      (7,5): ('L','R','D'),\n",
    "      (7,1): ('L','U','D'),\n",
    "      (6,2): ('L','R','U'),\n",
    "      (7,3): ('R','U','D'),\n",
    "      (8,2): (('L','R','D'))       \n",
    "  })\n",
    "  del actions[(6,5)]\n",
    "  del actions[(7,2)]\n",
    "  \n",
    "  g.set(rewards, actions, obey_prob)\n",
    "  if step_cost is not None:\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,10):\n",
    "                if (i,j) not in g.rewards.keys():\n",
    "                    g.rewards.update({\n",
    "                        (i,j): step_cost\n",
    "                    })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03| 1.00|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-1.00|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.20|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03| 0.20|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "------------------------------------------------------------\n",
      "-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|-0.03|\n",
      "\n",
      "Values (Policy Iteration Results):\n",
      "------------------------------------------------------------\n",
      " 0.06| 0.11| 0.17| 0.23| 0.31| 0.39| 0.48| 0.59| 0.70| 0.83|\n",
      "------------------------------------------------------------\n",
      " 0.07| 0.12| 0.18| 0.24| 0.32| 0.41| 0.52| 0.64| 0.80| 1.00|\n",
      "------------------------------------------------------------\n",
      " 0.07| 0.12| 0.18| 0.24| 0.31| 0.38| 0.42| 0.52| 0.50|-1.00|\n",
      "------------------------------------------------------------\n",
      " 0.10| 0.16| 0.22| 0.31| 0.39| 0.47| 0.40| 0.41| 0.39| 0.20|\n",
      "------------------------------------------------------------\n",
      " 0.13| 0.18| 0.13| 0.39| 0.49| 0.60| 0.49| 0.40| 0.33| 0.25|\n",
      "------------------------------------------------------------\n",
      " 0.18| 0.26| 0.35| 0.47| 0.60| 0.75| 0.60| 0.47| 0.37| 0.28|\n",
      "------------------------------------------------------------\n",
      " 0.16| 0.23| 0.31| 0.38| 0.48|-0.03| 0.48| 0.38| 0.30| 0.23|\n",
      "------------------------------------------------------------\n",
      " 0.11| 0.16|-0.03| 0.31| 0.37| 0.28| 0.37| 0.30| 0.23| 0.17|\n",
      "------------------------------------------------------------\n",
      " 0.06| 0.10| 0.16| 0.23| 0.28| 0.22| 0.27| 0.23| 0.17| 0.12|\n",
      "------------------------------------------------------------\n",
      " 0.02| 0.07| 0.11| 0.16| 0.20| 0.16| 0.20| 0.16| 0.11| 0.07|\n",
      "\n",
      "Policy (Policy Iteration Results):\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  D  |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |     |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  D  |  D  |  R  |  U  |  U  |     |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  D  |  D  |  D  |  U  |  U  |  D  |\n",
      "------------------------------------------------------------\n",
      "  D  |  D  |  R  |  R  |  R  |  D  |  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  L  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  U  |     |  U  |  L  |  L  |  L  |\n",
      "------------------------------------------------------------\n",
      "  U  |  U  |     |  U  |  U  |  L  |  U  |  U  |  U  |  U  |\n",
      "------------------------------------------------------------\n",
      "  U  |  R  |  R  |  U  |  U  |  U  |  U  |  U  |  U  |  U  |\n",
      "------------------------------------------------------------\n",
      "  R  |  R  |  R  |  U  |  U  |  U  |  U  |  U  |  L  |  U  |\n",
      "\n",
      "Iterations:  29\n"
     ]
    }
   ],
   "source": [
    "    grid=huge_grid(obey_prob=0.8,step_cost=-0.03)\n",
    "    \n",
    "    print('rewards:')\n",
    "    print_values(grid.rewards,grid)\n",
    "    print('')\n",
    "    \n",
    "    V,iteration,policy=policy_iteration(grid)\n",
    "    \n",
    "    print('Values (Policy Iteration Results):')\n",
    "    print_values(V,grid)\n",
    "    print('')\n",
    "    print('Policy (Policy Iteration Results):')\n",
    "    print_policy(policy,grid)\n",
    "    print('')\n",
    "    print('Iterations: ',iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28945749962376244"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(V.values())/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100\n",
    "max_steps=1000\n",
    "rewards=[]\n",
    "final_states=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps):\n",
    "        action=policy[state]\n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "        if done ==True:\n",
    "            break\n",
    "    rewards.append(rewards_current_episode)\n",
    "    final_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.48129999999996"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(1, 9): 47,\n",
       "         (5, 5): 29,\n",
       "         (5, 6): 15,\n",
       "         (6, 6): 2,\n",
       "         (4, 5): 4,\n",
       "         (4, 6): 1,\n",
       "         (4, 4): 1,\n",
       "         (3, 6): 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(final_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
