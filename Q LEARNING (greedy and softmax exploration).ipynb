{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values(V,g):\n",
    "    for i in range(g.width):\n",
    "        print('------------------------------------------------------------')\n",
    "        for j in range(g.height):\n",
    "            v=V.get((i,j),0)\n",
    "            if v>=0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print('')\n",
    "\n",
    "def print_policy(P,g):\n",
    "    for i in range(g.width):\n",
    "        print('-------------------------------------------------------------')\n",
    "        for j in range(g.height):\n",
    "            a=P.get((i,j),' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print('')\n",
    "        \n",
    "def max_dict(d):\n",
    "    max_key=None\n",
    "    max_val=float('-inf')\n",
    "    for k,v in d.itmes():\n",
    "        if v>max_val:\n",
    "            max_val=v\n",
    "            max_key=k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid: # Environment\n",
    "  def __init__(self, width, height, start):\n",
    "    # i is vertical axis, j is horizontal\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0]\n",
    "    self.j = start[1]\n",
    "    self.start=start\n",
    "\n",
    "  def set(self, rewards, actions, obey_prob):\n",
    "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "    self.obey_prob = obey_prob\n",
    "\n",
    "  def non_terminal_states(self):\n",
    "    return self.actions.keys()\n",
    "\n",
    "  def terminal_states(self):\n",
    "    return [x for x in self.rewards.keys() if x not in self.actions.keys()]\n",
    "\n",
    "  def set_state(self, s):\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "\n",
    "  def current_state(self):\n",
    "    return (self.i, self.j)\n",
    "\n",
    "  def is_terminal(self, s):\n",
    "    return s not in self.actions\n",
    "\n",
    "  def stochastic_move(self, action):\n",
    "    p = np.random.random()\n",
    "    if p <= self.obey_prob:\n",
    "      return action\n",
    "    if action == 'U' or action == 'D':\n",
    "      return np.random.choice(['L', 'R'])\n",
    "    elif action == 'L' or action == 'R':\n",
    "      return np.random.choice(['U', 'D'])\n",
    "\n",
    "  def move(self, action):\n",
    "    actual_action = self.stochastic_move(action)\n",
    "    if actual_action in self.actions[(self.i, self.j)]:\n",
    "      if actual_action == 'U':\n",
    "        self.i -= 1\n",
    "      elif actual_action == 'D':\n",
    "        self.i += 1\n",
    "      elif actual_action == 'R':\n",
    "        self.j += 1\n",
    "      elif actual_action == 'L':\n",
    "        self.j -= 1\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "  def step(self,action):\n",
    "    actual_action=self.stochastic_move(action)\n",
    "    new_state=[self.i,self.j]\n",
    "    if actual_action in self.actions[(self.i, self.j)]:\n",
    "      if actual_action == 'U':\n",
    "        new_state[0] -= 1\n",
    "      elif actual_action == 'D':\n",
    "        new_state[0] += 1\n",
    "      elif actual_action == 'R':\n",
    "        new_state[1] += 1\n",
    "      elif actual_action == 'L':\n",
    "        new_state[1] -= 1\n",
    "    new_state=tuple(new_state)\n",
    "    reward=self.rewards[new_state]\n",
    "    done=self.is_terminal(new_state)\n",
    "    return new_state,reward,done\n",
    "            \n",
    "\n",
    "  def check_move(self, action):\n",
    "    i = self.i\n",
    "    j = self.j\n",
    "    # check if legal move first\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        i -= 1\n",
    "      elif action == 'D':\n",
    "        i += 1\n",
    "      elif action == 'R':\n",
    "        j += 1\n",
    "      elif action == 'L':\n",
    "        j -= 1\n",
    "    # return a reward (if any)\n",
    "    return (i,j)\n",
    "\n",
    "  def get_transition_probs(self, action):\n",
    "    # returns a list of (probability, reward, s') transition tuples\n",
    "    probs = []\n",
    "    state = self.check_move(action)\n",
    "    probs.append((self.obey_prob, state))\n",
    "    disobey_prob = 1 - self.obey_prob\n",
    "    if not (disobey_prob > 0.0):\n",
    "      return probs\n",
    "    if action == 'U' or action == 'D':\n",
    "      state = self.check_move('L')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "      state = self.check_move('R')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "    elif action == 'L' or action == 'R':\n",
    "      state = self.check_move('U')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "      state = self.check_move('D')\n",
    "      probs.append((disobey_prob / 2, state))\n",
    "    return probs\n",
    "\n",
    "  def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if we are in a state where no actions are possible\n",
    "    return (self.i, self.j) not in self.actions\n",
    "\n",
    "  def all_states(self):\n",
    "    # possibly buggy but simple way to get all states\n",
    "    # either a position that has possible next actions\n",
    "    # or a position that yields a reward\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid(obey_prob=1.0, step_cost=None):\n",
    "  # define a grid that describes the reward for arriving at each state\n",
    "  # and possible actions at each state\n",
    "  # the grid looks like this\n",
    "  # x means you can't go there\n",
    "  # s means start position\n",
    "  # number means reward at that state\n",
    "  # .  .  .  1\n",
    "  # .  x  . -1\n",
    "  # s  .  .  .\n",
    "  # obey_brob (float): the probability of obeying the command\n",
    "  # step_cost (float): a penalty applied each step to minimize the number of moves (-0.1)\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions, obey_prob)\n",
    "  if step_cost is not None:\n",
    "    g.rewards.update({\n",
    "      (0, 0): step_cost,\n",
    "      (0, 1): step_cost,\n",
    "      (0, 2): step_cost,\n",
    "      (1, 0): step_cost,\n",
    "      (1, 2): step_cost,\n",
    "      (2, 0): step_cost,\n",
    "      (2, 1): step_cost,\n",
    "      (2, 2): step_cost,\n",
    "      (2, 3): step_cost,\n",
    "    })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=10000\n",
    "max_steps_per_episode=1000\n",
    "\n",
    "learning_rate=0.1\n",
    "discount_rate=0.9\n",
    "\n",
    "exploration_rate=1\n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=standard_grid(obey_prob=0.8, step_cost=-0.1)\n",
    "q_table={}\n",
    "for i in grid.actions.keys():\n",
    "    q_table.update({\n",
    "            i:{}\n",
    "        })\n",
    "for i in grid.actions.keys():\n",
    "    for j in grid.actions[i]:\n",
    "        q_table[i].update({\n",
    "            j:0\n",
    "        })\n",
    "        \n",
    "rewards_all_episodes=[]\n",
    "Steps=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate_shreshold=random.uniform(0,1)\n",
    "        if exploration_rate_shreshold>exploration_rate:\n",
    "            max_reward=float('-inf')\n",
    "            for i in q_table[state].keys():\n",
    "                if q_table[state][i]>max_reward:\n",
    "                    max_reward=q_table[state][i]\n",
    "                    action=i\n",
    "        else:\n",
    "            action=random.sample(grid.actions[state],1)[0]\n",
    "        \n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        \n",
    "        if done==True:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward)\n",
    "        \n",
    "        else:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward+discount_rate*max(q_table[new_state].values()))\n",
    "\n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "\n",
    "        if done ==True:\n",
    "            break\n",
    "    Steps.append(step)\n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate - min_exploration_rate) *\\\n",
    "                        np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.381"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Steps[-1000:])/len(Steps[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 :  -0.7982000000000002\n",
      "2000 :  0.11880000000000034\n",
      "3000 :  0.31079999999999935\n",
      "4000 :  0.38749999999999957\n",
      "5000 :  0.35689999999999933\n",
      "6000 :  0.41649999999999854\n",
      "7000 :  0.3960999999999988\n",
      "8000 :  0.3805999999999989\n",
      "9000 :  0.39569999999999866\n",
      "10000 :  0.39419999999999933\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count=1000\n",
    "rewards_thousand_episodes=[]\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    rewards_thousand_episodes.append(sum(r/1000))\n",
    "    print(count,\": \",str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy={}\n",
    "for i in q_table.keys():\n",
    "    max_q=float('-inf')\n",
    "    for j in q_table[i].keys():\n",
    "        if q_table[i][j]>max_q:\n",
    "            max_q=q_table[i][j]\n",
    "            policy.update({\n",
    "                i:j\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy (Q LEARNING Results):\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "-------------------------------------------------------------\n",
      "  U  |     |  U  |     |\n",
      "-------------------------------------------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "    print('Policy (Q LEARNING Results):')\n",
    "    print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huge_grid(obey_prob=1.0, step_cost=None):\n",
    "\n",
    "  g = Grid(10, 10, (9, 0))\n",
    "  rewards = {(1, 9): 1, (2, 9): -1,(4,2):-0.2, (5,5):0.2}\n",
    "  actions = {\n",
    "      (0,0): ('R','D'),\n",
    "      (0,9): ('L','D'),\n",
    "      (9,0): ('R','U'),\n",
    "      (9,9): ('L','U'),\n",
    "  }\n",
    "  for j in range(1,9):\n",
    "      actions[(0,j)]=('L','R','D')\n",
    "      actions[(9,j)]=('L','R','U')\n",
    "  for i in range(1,9):\n",
    "      actions[(i,0)]=('R','U','D')\n",
    "  for i in range(3,9):\n",
    "      actions[(i,9)]=('L','U','D')\n",
    "  for i in range(1,9):\n",
    "    for j in range(1,9):\n",
    "        actions[(i,j)]=('L','R','U','D')\n",
    "  actions.update({\n",
    "      (6,4): ('L','U','D'),\n",
    "      (5,5): ('L','R','U'),\n",
    "      (6,6): ('R','U','D'),\n",
    "      (7,5): ('L','R','D'),\n",
    "      (7,1): ('L','U','D'),\n",
    "      (6,2): ('L','R','U'),\n",
    "      (7,3): ('R','U','D'),\n",
    "      (8,2): (('L','R','D'))       \n",
    "  })\n",
    "  del actions[(6,5)]\n",
    "  del actions[(7,2)]\n",
    "  \n",
    "  g.set(rewards, actions, obey_prob)\n",
    "  if step_cost is not None:\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,10):\n",
    "                if (i,j) not in g.rewards.keys():\n",
    "                    g.rewards.update({\n",
    "                        (i,j): step_cost\n",
    "                    })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=huge_grid(obey_prob=0.8, step_cost=-0.03)\n",
    "q_table={}\n",
    "for i in grid.actions.keys():\n",
    "    q_table.update({\n",
    "            i:{}\n",
    "        })\n",
    "for i in grid.actions.keys():\n",
    "    for j in grid.actions[i]:\n",
    "        q_table[i].update({\n",
    "            j:0\n",
    "        })\n",
    "        \n",
    "rewards_all_episodes=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        exploration_rate_shreshold=random.uniform(0,1)\n",
    "        if exploration_rate_shreshold>exploration_rate:\n",
    "            max_reward=float('-inf')\n",
    "            for i in q_table[state].keys():\n",
    "                if q_table[state][i]>max_reward:\n",
    "                    max_reward=q_table[state][i]\n",
    "                    action=i\n",
    "        else:\n",
    "            action=random.sample(grid.actions[state],1)[0]\n",
    "        \n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        \n",
    "        if done==True:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward)\n",
    "        \n",
    "        else:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward+discount_rate*max(q_table[new_state].values()))\n",
    "\n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "\n",
    "        if done ==True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate - min_exploration_rate) *\\\n",
    "                        np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 :  -0.7532199999999982\n",
      "2000 :  17.800919999999923\n",
      "3000 :  38.299079999999876\n",
      "4000 :  46.256870000000006\n",
      "5000 :  50.11518\n",
      "6000 :  52.695600000000034\n",
      "7000 :  52.33926999999996\n",
      "8000 :  50.698279999999855\n",
      "9000 :  51.51677000000006\n",
      "10000 :  52.20724000000002\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count=1000\n",
    "rewards_thousand_episodes=[]\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    rewards_thousand_episodes.append(sum(r/1000))\n",
    "    print(count,\": \",str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy={}\n",
    "for i in q_table.keys():\n",
    "    max_q=float('-inf')\n",
    "    for j in q_table[i].keys():\n",
    "        if q_table[i][j]>max_q:\n",
    "            max_q=q_table[i][j]\n",
    "            policy.update({\n",
    "                i:j\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy (Q LEARNING Results):\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  D  |\n",
      "-------------------------------------------------------------\n",
      "  D  |  D  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |     |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  U  |  U  |  L  |     |\n",
      "-------------------------------------------------------------\n",
      "  R  |  D  |  R  |  R  |  D  |  D  |  D  |  U  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  D  |  D  |  R  |  R  |  R  |  D  |  L  |  L  |  L  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  L  |  L  |  L  |  L  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  U  |     |  U  |  L  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  U  |  U  |     |  U  |  U  |  L  |  U  |  L  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  U  |  U  |  R  |  R  |  U  |  U  |  U  |  U  |  U  |  U  |\n",
      "-------------------------------------------------------------\n",
      "  U  |  R  |  R  |  R  |  U  |  U  |  U  |  U  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "    print('Policy (Q LEARNING Results):')\n",
    "    print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=[1,2,3,4,5,6,7,8,9,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episodes,rewards_thousand_episodes,'go')\n",
    "plt.ylabel('Average Rewards')\n",
    "plt.xlabel('Per 1k episodes')\n",
    "plt.title('Q Learning Large Grid')\n",
    "plt.savefig('E:/Machine Learning/HW4/QL_LG.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100\n",
    "max_steps=1000\n",
    "rewards=[]\n",
    "final_states=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps):\n",
    "        action=policy[state]\n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "        if done ==True:\n",
    "            break\n",
    "    rewards.append(rewards_current_episode)\n",
    "    final_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.13199999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=huge_grid(obey_prob=0.8, step_cost=-0.03)\n",
    "q_table={}\n",
    "Steps=[]\n",
    "for i in grid.actions.keys():\n",
    "    q_table.update({\n",
    "            i:{}\n",
    "        })\n",
    "for i in grid.actions.keys():\n",
    "    for j in grid.actions[i]:\n",
    "        q_table[i].update({\n",
    "            j:0\n",
    "        })\n",
    "        \n",
    "rewards_all_episodes=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        possible_actions=grid.actions[state]\n",
    "        sum_exp=sum(np.exp(list(q_table[state].values())))\n",
    "        probability_action=[]\n",
    "        for a in possible_actions:\n",
    "            probability_action.append(np.exp(q_table[state][a])/sum_exp)\n",
    "        action=random.choices(possible_actions, probability_action)[0]  \n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        \n",
    "        if done==True:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward)\n",
    "        \n",
    "        else:\n",
    "            q_table[state][action]=q_table[state][action]*(1-learning_rate)+\\\n",
    "               learning_rate*(reward+discount_rate*max(q_table[new_state].values()))\n",
    "\n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "\n",
    "        if done ==True:\n",
    "            Steps.append(step)\n",
    "            break\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 :  -8.54607999999999\n",
      "2000 :  -8.521959999999998\n",
      "3000 :  -8.955809999999994\n",
      "4000 :  -9.397570000000009\n",
      "5000 :  -8.670419999999996\n",
      "6000 :  -8.752449999999998\n",
      "7000 :  -8.80036999999999\n",
      "8000 :  -9.15067000000001\n",
      "9000 :  -8.620729999999996\n",
      "10000 :  -9.116660000000001\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count=1000\n",
    "rewards_thousand_episodes=[]\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    rewards_thousand_episodes.append(sum(r/1000))\n",
    "    print(count,\": \",str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy={}\n",
    "for i in q_table.keys():\n",
    "    max_q=float('-inf')\n",
    "    for j in q_table[i].keys():\n",
    "        if q_table[i][j]>max_q:\n",
    "            max_q=q_table[i][j]\n",
    "            policy.update({\n",
    "                i:j\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy (Q LEARNING Results, Softmax Exploration):\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  D  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |     |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  D  |  D  |  D  |  R  |  U  |  U  |     |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  D  |  D  |  L  |  U  |  U  |  D  |\n",
      "-------------------------------------------------------------\n",
      "  D  |  D  |  R  |  D  |  D  |  D  |  L  |  L  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  R  |  L  |  L  |  L  |  L  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  U  |     |  U  |  U  |  L  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  U  |     |  U  |  U  |  L  |  U  |  U  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  U  |  R  |  U  |  U  |  U  |  U  |  U  |  U  |  L  |\n",
      "-------------------------------------------------------------\n",
      "  R  |  R  |  U  |  U  |  U  |  U  |  U  |  U  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "    print('Policy (Q LEARNING Results, Softmax Exploration):')\n",
    "    print_policy(policy,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100\n",
    "max_steps=1000\n",
    "rewards=[]\n",
    "final_states=[]\n",
    "for episode in range(num_episodes):\n",
    "    grid.set_state(grid.start)\n",
    "    state=(grid.i,grid.j)\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps):\n",
    "        action=policy[state]\n",
    "        new_state,reward,done=grid.step(action)    \n",
    "        state=new_state\n",
    "        grid.set_state(state)\n",
    "        rewards_current_episode+=reward\n",
    "        if done ==True:\n",
    "            break\n",
    "    rewards.append(rewards_current_episode)\n",
    "    final_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.99060000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(5, 4): 41,\n",
       "         (4, 4): 6,\n",
       "         (5, 5): 36,\n",
       "         (4, 5): 5,\n",
       "         (6, 4): 7,\n",
       "         (1, 9): 4,\n",
       "         (6, 3): 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(final_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes=[1,2,3,4,5,6,7,8,9,10]\n",
    "plt.plot(episodes,rewards_thousand_episodes,'go')\n",
    "plt.ylabel('Average Rewards')\n",
    "plt.xlabel('Per 1k episodes')\n",
    "plt.title('Q Learning Large Grid (Softmax Exploration) ')\n",
    "plt.savefig('E:/Machine Learning/HW4/QL_SE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.507"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Steps[-1000:])/1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
